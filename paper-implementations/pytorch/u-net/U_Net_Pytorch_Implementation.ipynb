{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-Net Pytorch Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHhVAMhy2/rF7khJfdHjhg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quickgrid/AI-Resources/blob/master/paper-implementations/pytorch/u-net/U_Net_Pytorch_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoNTuk5L22qU"
      },
      "source": [
        "# References:\n",
        "- https://www.youtube.com/watch?v=IHq1t7NxS8k&t=22s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktWw1nLCnP3g"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.conv(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le8Zq4TprFv6"
      },
      "source": [
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Downward path of UNET.\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels=in_channels, out_channels=feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Upward path of UNET.\n",
        "        # Transposed convolution is used to scale up image before applying double convolutions.\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(in_channels=feature * 2, out_channels=feature, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.ups.append(DoubleConv(in_channels=feature * 2, out_channels=feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(in_channels=features[-1], out_channels=features[-1] * 2)\n",
        "        self.final_conv = nn.Conv2d(in_channels=features[0], out_channels=out_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        skip_connections = []\n",
        "\n",
        "        # Create connections in downward path.\n",
        "        for down in self.downs:\n",
        "            X = down(X)\n",
        "            skip_connections.append(X)\n",
        "            X = self.pool(X)\n",
        "\n",
        "        X = self.bottleneck(X)\n",
        "\n",
        "        # Reverse the connections such that the last added skip connection is at 0th position.\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        # Upward connection of network to connect skip connections with upsample image.\n",
        "        # In ups list in odd positions are upsample ones and in even positions are double convolutions.\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            X = self.ups[idx](X)\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            # In case image shape is odd and rounded to integer then skip connection and X will not match.\n",
        "            # In this case the input is resized to match shape for concatenation.\n",
        "            if X.shape != skip_connection.shape:\n",
        "                X = F.resize(X, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, X), dim=1)\n",
        "            X = self.ups[idx + 1](concat_skip)\n",
        "\n",
        "        return self.final_conv(X)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCqwIZW53co"
      },
      "source": [
        "class Tester():\n",
        "    def __init__(self):\n",
        "        super(Tester, self).__init__()\n",
        "\n",
        "    def unet_test(self):\n",
        "        X = torch.randn((3, 1, 161, 161))\n",
        "        model = UNET(in_channels=1, out_channels=1)\n",
        "        preds = model(X)\n",
        "        print(X.shape)\n",
        "        print(preds.shape)\n",
        "        assert X.shape == preds.shape, \"Shape of input and predicted images did not match.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zx4Bo-153VI",
        "outputId": "a9c1fb37-fc0e-42de-8a18-6a905cd1fae6"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    Tester().unet_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 161, 161])\n",
            "torch.Size([3, 1, 161, 161])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCqiJDes6yng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2e0F174Zgus"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CarvanaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_dir,\n",
        "        mask_dir,\n",
        "        transform=None\n",
        "    ):\n",
        "        super(CarvanaDataset, self).__init__()\n",
        "        \n",
        "        self.image_dir = image_dir \n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = os.path.join(self.image_dir, self.images[index])\n",
        "\n",
        "        # Since train image and mask have similar names, but different extensions\n",
        "        # the masks can be loaded by replacing '.jpg' with mask ending name.\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index].replace('jpg', '_mask.gif'))\n",
        "        \n",
        "        image = np.array(Image.open(image_path).convert('RGB'))\n",
        "\n",
        "        # Masks are grayscale single channels images.\n",
        "        mask = np.array(Image.open(mask_path).convert('L'), dtype=np.float32)\n",
        "\n",
        "        # Since masks are black and white with value of 0 and 255. \n",
        "        # These values can be normalized by replacing white value of 255 to 1.\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        # Apply augmentation is available.\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}