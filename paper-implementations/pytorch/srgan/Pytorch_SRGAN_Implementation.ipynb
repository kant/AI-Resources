{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch SRGAN Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAaFBmNWmwkW75EVWrOYcM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quickgrid/AI-Resources/blob/master/paper-implementations/pytorch/srgan/Pytorch_SRGAN_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgUBldhqOtN_"
      },
      "source": [
        "### References\n",
        "- https://www.youtube.com/watch?v=7FO9qDOhRCc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tYD9-d92YQM"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            discriminator=False,\n",
        "            use_activation=True,\n",
        "            use_bn=True,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.use_activation = use_activation\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, **kwargs, bias=not use_bn)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels) if use_bn else nn.Identity()\n",
        "\n",
        "        # Section 2.1 explains leaky relu slope amount.\n",
        "        # Based on Figure 4, leaky relu is applied only is discriminator and prelu in generator.\n",
        "        self.act = (\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True) if discriminator\n",
        "            else nn.PReLU(num_parameters=out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.act(self.bn(self.conv(X))) if self.use_activation else self.bn(self.act(X))\n",
        "\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        scale_factor,\n",
        "    ):\n",
        "        super(UpSampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels * (scale_factor ** 2), kernel_size=3, stride=1, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
        "        self.activation = nn.PReLU(num_parameters=in_channels)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.activation(self.pixel_shuffle(self.conv(X)))\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels\n",
        "    ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block1 = ConvBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=in_channels,\n",
        "            discriminator=False,\n",
        "            use_activation=True,\n",
        "            use_bn=True,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "        )\n",
        "        self.block2 = ConvBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=in_channels,\n",
        "            discriminator=False,\n",
        "            use_activation=False,\n",
        "            use_bn=True,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        out = self.block1(X)\n",
        "        out = self.block2(out)\n",
        "        return X + out\n",
        "\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        num_channels=64,\n",
        "        num_blocks=16,\n",
        "        \n",
        "    ):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.initial = ConvBlock(in_channels=in_channels, out_channels=num_channels, kernel_size=9, stride=1, padding=4, use_bn=False)\n",
        "        self.residuals = nn.Sequential(*[ResidualBlock(in_channels=num_channels) for _ in range(num_blocks)])\n",
        "        self.conv_block = ConvBlock(\n",
        "            in_channels=num_channels,\n",
        "            out_channels=num_channels,\n",
        "            discriminator=False,\n",
        "            use_activation=False,\n",
        "            use_bn=True,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "        )\n",
        "        self.upsamples = nn.Sequential(\n",
        "            UpSampleBlock(in_channels=num_channels, scale_factor=2),\n",
        "            UpSampleBlock(in_channels=num_channels, scale_factor=2),\n",
        "        )\n",
        "        self.final = nn.Conv2d(in_channels=num_channels, out_channels=in_channels, kernel_size=9, stride=1, padding=4)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        initial = self.initial(X)\n",
        "        X = self.residuals(initial)\n",
        "        X = self.conv_block(X)\n",
        "        X = initial + X\n",
        "        X = self.upsamples(X)\n",
        "        X = self.final(X)\n",
        "        return torch.tanh(X)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        features=[64, 64, 128, 128, 256, 256, 512, 512]\n",
        "    ):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        blocks = []\n",
        "        for idx, feature in enumerate(features):\n",
        "            blocks.append(\n",
        "                ConvBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=feature,\n",
        "                    discriminator=True,\n",
        "                    use_activation=True,\n",
        "                    use_bn=False if idx == 0 else True,\n",
        "                    kernel_size=3,\n",
        "                    padding=1,\n",
        "                    stride=1 + idx % 2\n",
        "                )\n",
        "            )\n",
        "            in_channels = feature\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(output_size=(6, 6)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=512 * 6 * 6, out_features=1024),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Linear(in_features=1024, out_features=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.blocks(X)\n",
        "        X = self.classifier(X)\n",
        "        return X"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xGBh3-7LmXP"
      },
      "source": [
        "def test():\n",
        "    low_resolution = 30\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        X = torch.randn((5, 3, low_resolution, low_resolution))\n",
        "        gen = Generator()\n",
        "        gen_out = gen(X)\n",
        "        disc = Discriminator()\n",
        "        disc_out = disc(gen_out)\n",
        "\n",
        "        print(gen_out.shape)\n",
        "        print(disc_out.shape)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-H9KXULLf31",
        "outputId": "aeab110d-2138-4c2c-a9ac-cd3f151865f2"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3, 120, 120])\n",
            "torch.Size([5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3F19DB7Mutd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}